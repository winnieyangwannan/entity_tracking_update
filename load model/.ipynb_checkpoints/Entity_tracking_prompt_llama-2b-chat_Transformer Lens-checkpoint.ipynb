{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744c89ce-25ab-478c-806b-23abac8c94d1",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is sanity check to see if transformer lens can replicate what I have in this [notebook](https://colab.research.google.com/drive/1nFX9O8ahmtJT2jIL9gMDVdNQNg61BsEM?usp=sharing), which was done without the transformer lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fea2207-9a65-4896-b1dc-f140b90143e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c361f6-a086-4858-81c6-a076c16fc25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "781004f9-54fd-4f2e-9203-fe892d2e8411",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "\n",
    "- Load the model and tokenizer **locally**\n",
    "- otherwise not compatible with the transformer lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb19a41c-6113-4641-b23b-0ff311074162",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_PATH = \"D:/Data/Llama/Llama_2/7b_chat_hf\"\n",
    "LLANA_NAME = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b695e394-6b90-4c0c-b7d3-5024639dca87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2e9cf7e8754c34a7025f6689ac4266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_hf = AutoModelForCausalLM.from_pretrained(LLAMA_PATH,\n",
    "                                               device_map='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adff258-ab7a-493f-9c06-88287fa57d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531d06208dfc4f12bdb670f805a77d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "inference_dtype = torch.float32\n",
    "\n",
    "model = HookedTransformer.from_pretrained(LLANA_NAME,\n",
    "                                             hf_model=model_hf,\n",
    "                                             dtype=inference_dtype,\n",
    "                                             fold_ln=False,\n",
    "                                             fold_value_biases=False,\n",
    "                                             center_writing_weights=False,\n",
    "                                             center_unembed=False,\n",
    "                                             tokenizer=tokenizer)\n",
    "model.generate(\"The capital of Germany is\", max_new_tokens=2, temperature=0)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a41be8-cd3e-403d-abb4-5b08c7eb50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 0.0 total(Gb): 11.81089792\n"
     ]
    }
   ],
   "source": [
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83645a08-8ffd-451a-8ce8-bc99057594d5",
   "metadata": {},
   "source": [
    "# Check performance in entity tracking with 1 state update\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n",
    "\n",
    "\n",
    "- The task structure is:\n",
    "  - Three boxes\n",
    "  - One state update\n",
    "  - Maximum of 1 object per box\n",
    "  - Without exact template\n",
    "\n",
    "\n",
    "- Comparing two kinds of prompt structure:\n",
    "  - 1. Few-shot prompt (without CoT)\n",
    "  - 2. Few-shot CoT + think step by step\n",
    "  - 3. zero-shot CoT (think step by step)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6661257-7f55-4a4c-ba62-0c2012b7f80b",
   "metadata": {},
   "source": [
    "## Zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ec69b7-0711-46f8-be64-121bb237f192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b1822a3e844bc38e38232b07500d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
      "\n",
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "\n",
      "Statement 3: Let's think step by step. Box A contains the cow, and John moves the cow to Box B. So, Box A is now empty, and Box B contains the cow.\n",
      "\n",
      "Box A: Empty\n",
      "Box B: Cow\n",
      "Box C: Mouse\n",
      "\n",
      "Please update the contents of the boxes according to the statements.</s>\n",
      "--- 118.9385597705841 seconds ---\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
    "\n",
    "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
    "\n",
    "Statement 3: Let's think step by step. Box A contains\"\"\"\n",
    "start_time = time.time()\n",
    "outputs = model.generate(prompt, max_new_tokens=100, temperature=0)\n",
    "print(outputs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8761fdab-aab9-4a44-b60e-82430576fa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b1822a3e844bc38e38232b07500d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
      "\n",
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "\n",
      "Statement 3: Let's think step by step. Box A contains the cow, and John moves the cow to Box B. So, Box A is now empty, and Box B contains the cow.\n",
      "\n",
      "Box A: Empty\n",
      "Box B: Cow\n",
      "Box C: Mouse\n",
      "\n",
      "Please update the contents of the boxes according to the statements.</s>\n",
      "--- 118.9385597705841 seconds ---\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
    "\n",
    "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
    "\n",
    "Statement 3: Let's think step by step. Box A contains\"\"\"\n",
    "start_time = time.time()\n",
    "outputs = model.generate(prompt, max_new_tokens=100, temperature=0)\n",
    "print(outputs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e547f4-5e6d-4605-9df7-cd1572f7cd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15287bcc4874a288b7fa0dbc0348198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
      "\n",
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "\n",
      "Statement 3: Let's think step by step. Box A contains the cow, and John moves the cow to Box B. So, Box A is now empty, and Box B contains the cow.\n",
      "\n",
      "Box A: Empty\n",
      "Box B: Cow\n",
      "Box C: Mouse\n",
      "\n",
      "Please update the contents of the boxes according to the statements.</s>\n",
      "--- 127.31756210327148 seconds ---\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
    "\n",
    "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
    "\n",
    "Statement 3: Let's think step by step. Box A contains\"\"\"\n",
    "start_time = time.time()\n",
    "outputs = model.generate(prompt, max_new_tokens=100, temperature=0)\n",
    "print(outputs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c27700c-048a-4d3f-b174-3824a0b889a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e4e0c2b8344bd92dbdb148323a4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:03<00:00, 123.60s/it]\n",
      "1it [02:03, 123.60s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "max_new_tokens = 100\n",
    "prompt = \"\"\"\n",
    "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
    "\n",
    "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
    "\n",
    "Statement 3: Let's think step by step. Box A contains\"\"\"\n",
    "\n",
    "input_tokens = model.to_tokens(prompt)\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\":input_tokens,\n",
    "    }).with_format(\"torch\")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, inputs in tqdm.tqdm(enumerate(tqdm.tqdm(dataloader))):\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].to('cuda')\n",
    "        # outputs = model(input_ids = inputs[\"input_ids\"]) # next token prediction\n",
    "        output = model.generate(inputs[\"input_ids\"],\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                temperature=0)  # generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b068f1d-bec7-47e7-a322-1022568b5a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Given the description after \"Description:\", write a true statement about all boxes and their contents after \"Statement:\". Make sure to keep track of the changes and update the contents of the boxes according to the changes.\n",
      "\n",
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "\n",
      "Statement 3: Let's think step by step. Box A contains the cow, and John moves the cow to Box B. So, Box A is now empty, and Box B contains the cow.\n",
      "\n",
      "Box A: Empty\n",
      "Box B: Cow\n",
      "Box C: Mouse\n",
      "\n",
      "Please update the contents of the boxes according to the statements.</s>\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552557a5-bbb9-4345-b815-bc9be23b24c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c33df3da-90e3-4a59-9153-d0a00e9e0dd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b2b56-c55c-4f1a-b698-2190448e0626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
