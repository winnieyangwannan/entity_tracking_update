{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e1ac6e-9404-4efd-aeb2-69b12d3fbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import numpy as np\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1801a5d-e202-4fe6-8d4c-e1ab93ec4fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x198ac164a70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd79c37-2509-4f1f-b69a-fef9c3605336",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2587e058-8260-4612-ac39-73e288784e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\\nStatement 3: Let's think step by step. Initially, box A contains the cow, and John moves the cow to Box B. So, Box A now contains nothing, and Box B contains the cow. Box C contains the mouse.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7600649a-7c02-4122-97ee-5d3ec3bb78f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "Statement 3: Let's think step by step. Initially, box A contains the cow, and John moves the cow to Box B. So, Box A now contains nothing, and Box B contains the cow. Box C contains the mouse.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c98bab-13ee-48ce-ac33-3673c0aa466b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "038598bf-2c27-4bc1-8e31-f8a0b8d96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name =\"stanford-crfm/alias-gpt2-small-x21\"\n",
    "token = \"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\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3afc200a-67d4-4548-9df6-d9fa7645b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "    def __init__(self, token, device=None, load_in_8bit=False):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name, token=token,\n",
    "                                                          device_map='auto')\n",
    "        self.model = HookedTransformer.from_pretrained(model_name,\n",
    "                                             hf_model=hf_model,\n",
    "                                             fold_ln=False,\n",
    "                                             fold_value_biases=False,\n",
    "                                             center_writing_weights=False,\n",
    "                                             center_unembed=False,\n",
    "                                             tokenizer=self.tokenizer,\n",
    "                                             device = self.device)\n",
    "\n",
    "        print(self.model)\n",
    "        print(self.model.cfg.n_layers)\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.d_vocab = self.model.cfg.d_vocab\n",
    "        self.n_layers = self.model.cfg.n_layers\n",
    "\n",
    "    def logits_all_layers(self, text):\n",
    "        inputs = self.tokenizer(text,return_tensors=\"pt\")\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Get residual output for each layer\n",
    "        z_name_filter = lambda name: name.endswith(\"resid_post\")\n",
    "        self.model.reset_hooks()\n",
    "        _,cache = self.model.run_with_cache(\n",
    "        inputs[\"input_ids\"],\n",
    "        names_filter = z_name_filter,\n",
    "        return_type = None\n",
    "        )\n",
    "        \n",
    "        layer_logit_all = torch.zeros(self.n_layers,seq_len,self.d_vocab)\n",
    "        for layer in range(self.model.cfg.n_layers):\n",
    "            resid_ln = self.model.ln_final(cache[f'blocks.{layer}.hook_resid_post'])\n",
    "            layer_logit = self.model.unembed(resid_ln)\n",
    "            layer_logit_all[layer,:,:] = layer_logit\n",
    "        return layer_logit_all\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "305b8d3a-ba47-4e60-beab-ed1ae9aedb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model stanford-crfm/alias-gpt2-small-x21 into HookedTransformer\n",
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "model = ModelHelper(token, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4051ba5f-fbf8-48c6-8628-b2f383b00b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelHelper' object has no attribute 'd_vcab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_all_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mlog_softmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[58], line 39\u001b[0m, in \u001b[0;36mModelHelper.logits_all_layers\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[0;32m     33\u001b[0m _,cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrun_with_cache(\n\u001b[0;32m     34\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     35\u001b[0m names_filter \u001b[38;5;241m=\u001b[39m z_name_filter,\n\u001b[0;32m     36\u001b[0m return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 39\u001b[0m layer_logit_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers,seq_len,\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_vcab\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m     41\u001b[0m     resid_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mln_final(cache[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelHelper' object has no attribute 'd_vcab'"
     ]
    }
   ],
   "source": [
    "log_probs = model.logits_all_layers(prompt).float().log_softmax(dim=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688bd71-1f30-4b69-ad7f-b428ef7365e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
