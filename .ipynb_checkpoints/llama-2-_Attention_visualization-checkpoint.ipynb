{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393c7df3-b6d5-4e25-99eb-75e1fb6fd840",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069b6780-5ed2-4b1f-964a-c06dfa38fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import numpy as np\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0204e4-d45c-486d-857e-ba7b9e0bc617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x19cee7b78f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2bf4b-7912-45f5-a8f9-3f8014efbfb2",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff5af17-f25c-4508-8334-1d1722765bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_PATH_7b_chat = \"D:/Data/Llama/Llama_2/7b_chat_hf\"\n",
    "LLANA_NAME_7b_chat = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "LLAMA_PATH_7b = \"D:/Data/Llama/Llama_2/7b_hf\"\n",
    "LLANA_NAME_7b = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "inference_type = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50024e50-0f07-492b-a0e3-c7c07da2a11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9afa7c9d17b4106913c7ddefff59c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_7b = AutoModelForCausalLM.from_pretrained(LLAMA_PATH_7b,\n",
    "                                               low_cpu_mem_usage=True)\n",
    "tokenizer_7b = AutoTokenizer.from_pretrained(LLAMA_PATH_7b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfce078-2aab-48b1-86f0-55b70f9b57f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afb507753484b4a8c9742c86b37e18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_7b_chat = AutoModelForCausalLM.from_pretrained(LLAMA_PATH_7b_chat,\n",
    "                                                     low_cpu_mem_usage=True)\n",
    "tokenizer_7b_chat = AutoTokenizer.from_pretrained(LLAMA_PATH_7b_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d524f0d-9ec7-4b4a-b986-df0b0b265ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(GB): 10.708058112 total(GB): 11.81089792\n",
      "CPU percentage: 13.7\n"
     ]
    }
   ],
   "source": [
    "print(\"free(GB):\", torch.cuda.mem_get_info()[0]/1000000000,\n",
    "     \"total(GB):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n",
    "print(\"CPU percentage:\", psutil.cpu_percent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc7b84-8da3-496a-a3b2-33e38f64745d",
   "metadata": {},
   "source": [
    "## Hook with Transformer Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef05771-da4b-4c9a-9be8-3f8e3840ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_tl_7b = HookedTransformer.from_pretrained(LLANA_NAME_7b,\n",
    "                                             hf_model=model_7b,\n",
    "                                             fold_ln=False,\n",
    "                                             fold_value_biases=False,\n",
    "                                             center_writing_weights=False,\n",
    "                                             center_unembed=False,\n",
    "                                             tokenizer=tokenizer_7b,\n",
    "                                             device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb650f9-783b-4e98-acb3-7a0e1534a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(GB): 10.708058112 total(GB): 11.81089792\n",
      "CPU percentage: 1.7\n"
     ]
    }
   ],
   "source": [
    "print(\"free(GB):\", torch.cuda.mem_get_info()[0]/1000000000,\n",
    "     \"total(GB):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n",
    "print(\"CPU percentage:\", psutil.cpu_percent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f9dfe9-d7cf-4aec-b15c-e87d4d9d88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model_tl_7b = model_tl_7b.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce60bd5b-6b70-4004-b94b-d08056bc39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(GB): 0.0 total(GB): 11.81089792\n",
      "CPU percentage: 9.1\n"
     ]
    }
   ],
   "source": [
    "print(\"free(GB):\", torch.cuda.mem_get_info()[0]/1000000000,\n",
    "     \"total(GB):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n",
    "print(\"CPU percentage:\", psutil.cpu_percent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3841bc-4530-4b4d-8632-db55dbd72357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "inference_dtype = torch.float32\n",
    "\n",
    "model_tl_7b_chat = HookedTransformer.from_pretrained(LLANA_NAME_7b_chat,\n",
    "                                             hf_model=model_7b_chat,\n",
    "                                             fold_ln=False,\n",
    "                                             fold_value_biases=False,\n",
    "                                             center_writing_weights=False,\n",
    "                                             center_unembed=False,\n",
    "                                             tokenizer=tokenizer_7b_chat,\n",
    "                                             device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee51dea8-7f97-4a2f-9caa-6ec277911813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(GB): 0.589402112 total(GB): 11.81089792\n",
      "CPU percentage: 1.4\n"
     ]
    }
   ],
   "source": [
    "print(\"free(GB):\", torch.cuda.mem_get_info()[0]/1000000000,\n",
    "     \"total(GB):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n",
    "print(\"CPU percentage:\", psutil.cpu_percent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fb9a3-ae5a-49ae-bfa9-6dea5a4c8a76",
   "metadata": {},
   "source": [
    "# Visualize Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1b5342-8495-44a0-88c0-08c51a06a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.ln1.w torch.Size([4096])\n",
      "blocks.0.ln2.w torch.Size([4096])\n",
      "blocks.0.attn.W_Q torch.Size([32, 4096, 128])\n",
      "blocks.0.attn.W_O torch.Size([32, 128, 4096])\n",
      "blocks.0.attn.b_Q torch.Size([32, 128])\n",
      "blocks.0.attn.b_O torch.Size([4096])\n",
      "blocks.0.attn.W_K torch.Size([32, 4096, 128])\n",
      "blocks.0.attn.W_V torch.Size([32, 4096, 128])\n",
      "blocks.0.attn.b_K torch.Size([32, 128])\n",
      "blocks.0.attn.b_V torch.Size([32, 128])\n",
      "blocks.0.mlp.W_in torch.Size([4096, 11008])\n",
      "blocks.0.mlp.W_gate torch.Size([4096, 11008])\n",
      "blocks.0.mlp.W_out torch.Size([11008, 4096])\n",
      "blocks.0.mlp.b_in torch.Size([11008])\n",
      "blocks.0.mlp.b_out torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (name, param) in model_tl_7b.named_parameters():\n",
    "    if name.startswith(f\"blocks.{0}.\"):\n",
    "        print(name, param.shape)\n",
    "        # param_diff = np.append(param_diff,(param-param_chat))\n",
    "        # param_diff_all[layer] = param_diff\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6f179-37a1-4189-b5a4-f10571fdfac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb0cd1-5dd9-453c-bdf0-c68ffc591d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93fd8e66-42a4-4c2d-84b9-aab04fde33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" The interaction between hippocampus and cortex is a translation problem.Transformer can be used for translation, thus we can use it to study the interaction between hippocampus and cortex.\"\n",
    "inputs = model_tl_7b.to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d508d7-2727-4318-a703-2a5c4a7425db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache =  model_tl_7b.run_with_cache(inputs, remove_batch_dim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598a893-626f-471d-a18f-15034bd9aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 5\n",
    "attention_pattern = cache[\"pattern\", layer] # get the attention pattern form cache\n",
    "print(\"attention_pattern:\",attention_pattern.shape)\n",
    "str_tokens = model_tl_7b.to_str_tokens(text) # to str tokens\n",
    "\n",
    "print(f\"Layer {layer} Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(\n",
    "    tokens=str_tokens, # notice the input to the cv need to be str_tokens\n",
    "    attention=attention_pattern,\n",
    "    attention_head_names=[f\"L{layer}H{i}\" for i in range(12)],\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
