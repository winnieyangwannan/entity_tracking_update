{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28e1ac6e-9404-4efd-aeb2-69b12d3fbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import numpy as np\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import random\n",
    "import tuned_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1801a5d-e202-4fe6-8d4c-e1ab93ec4fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x198ac164a70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd79c37-2509-4f1f-b69a-fef9c3605336",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2587e058-8260-4612-ac39-73e288784e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\\nStatement 3: Let's think step by step. Initially, box A contains the cow, and John moves the cow to Box B. So, Box A now contains nothing, and Box B contains the cow. Box C contains the mouse.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7600649a-7c02-4122-97ee-5d3ec3bb78f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
      "Statement 3: Let's think step by step. Initially, box A contains the cow, and John moves the cow to Box B. So, Box A now contains nothing, and Box B contains the cow. Box C contains the mouse.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c98bab-13ee-48ce-ac33-3673c0aa466b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "038598bf-2c27-4bc1-8e31-f8a0b8d96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name =\"stanford-crfm/alias-gpt2-small-x21\"\n",
    "token = \"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\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3afc200a-67d4-4548-9df6-d9fa7645b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "    def __init__(self, token, device=None, load_in_8bit=False):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name, token=token,\n",
    "                                                          device_map='auto')\n",
    "        self.model = HookedTransformer.from_pretrained(model_name,\n",
    "                                             hf_model=hf_model,\n",
    "                                             fold_ln=False,\n",
    "                                             fold_value_biases=False,\n",
    "                                             center_writing_weights=False,\n",
    "                                             center_unembed=False,\n",
    "                                             tokenizer=self.tokenizer,\n",
    "                                             device = self.device)\n",
    "\n",
    "        print(self.model)\n",
    "        print(self.model.cfg.n_layers)\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.d_vocab = self.model.cfg.d_vocab\n",
    "        self.n_layers = self.model.cfg.n_layers\n",
    "\n",
    "    def logits_all_layers(self, text):\n",
    "        inputs = self.tokenizer(text,return_tensors=\"pt\")\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Get residual output for each layer\n",
    "        z_name_filter = lambda name: name.endswith(\"resid_post\")\n",
    "        self.model.reset_hooks()\n",
    "        _,cache = self.model.run_with_cache(\n",
    "        inputs[\"input_ids\"],\n",
    "        names_filter = z_name_filter,\n",
    "        return_type = None\n",
    "        )\n",
    "        \n",
    "        layer_logit_all = torch.zeros(self.n_layers,seq_len,self.d_vocab)\n",
    "        for layer in range(self.model.cfg.n_layers):\n",
    "            resid_ln = self.model.ln_final(cache[f'blocks.{layer}.hook_resid_post'])\n",
    "            layer_logit = self.model.unembed(resid_ln)\n",
    "            layer_logit_all[layer,:,:] = layer_logit\n",
    "        return layer_logit_all\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "305b8d3a-ba47-4e60-beab-ed1ae9aedb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model stanford-crfm/alias-gpt2-small-x21 into HookedTransformer\n",
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "model = ModelHelper(token, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4051ba5f-fbf8-48c6-8628-b2f383b00b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = model.logits_all_layers(prompt).float().log_softmax(dim=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e688bd71-1f30-4b69-ad7f-b428ef7365e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "from tuned_lens.plotting import PredictionTrajectory\n",
    "import ipywidgets as widgets\n",
    "from plotly import graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "tokenizer=model.tokenizer\n",
    "def make_plot(text, layer_stride, statistic, token_range):\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    targets = input_ids[1:] + [tokenizer.eos_token_id]\n",
    "\n",
    "    if len(input_ids) == 0:\n",
    "        return widgets.Text(\"Please enter some text.\")\n",
    "\n",
    "    if (token_range[0] == token_range[1]):\n",
    "        return widgets.Text(\"Please provide valid token range.\")\n",
    "\n",
    "    print(text)\n",
    "    print(model.logits_all_layers(text).shape)\n",
    "    log_probs = model.logits_all_layers(text).float().log_softmax(dim=-1).numpy()\n",
    "    pred_traj = PredictionTrajectory(log_probs = log_probs,#np.zeros([32, len(input_ids), 32000]),\n",
    "                                    input_ids = np.asarray(input_ids),\n",
    "                                    targets= np.asarray(targets),\n",
    "                                    anti_targets=None,\n",
    "                                    tokenizer=tokenizer)\n",
    "    pred_traj = pred_traj.slice_sequence(slice(*token_range))\n",
    "    return getattr(pred_traj, statistic)().stride(layer_stride).figure(\n",
    "        title=f\"LLamav2lense {statistic}\",\n",
    "    )\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "statistic_wdg = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Entropy', 'entropy'),\n",
    "        ('Cross Entropy', 'cross_entropy'),\n",
    "        ('Forward KL', 'forward_kl'),\n",
    "    ],\n",
    "    description='Select Statistic:',\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "\n",
    "layer_stride_wdg = widgets.BoundedIntText(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Layer Stride:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "token_range_wdg = widgets.IntRangeSlider(\n",
    "    description='Token Range',\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=1,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "\n",
    "# def update_token_range(response_len=1,*args):\n",
    "#     token_range_wdg.max = len(tokenizer.encode(text_wdg.value))+ response_len\n",
    "\n",
    "def update_token_range(*args):\n",
    "    token_range_wdg.max = len(tokenizer.encode(text_wdg.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f13624ce-f9bf-4398-b000-a26a7ca15e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f92ce968eb4f23919bf9d3a375c41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value=\"\\n\\nDescription 3: Box A contains the cow. Box B contains nothing. Box C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Description 3: Box A contains the cow. Box B contains nothing. Box C contains the mouse. John moves the cow from Box A to Box B. Box C has no change in its content.\n",
    "\n",
    "Statement 3: Let's think step by step. Initially, box A contains the cow, and John moves the cow to Box B. So, Box A now contains nothing, and Box B contains the cow. Box C contains the mouse.\"\"\"\n",
    "text_wdg = widgets.Textarea(\n",
    "    description=\"Input Text\",\n",
    "    value =prompt\n",
    ")\n",
    "\n",
    "\n",
    "update_token_range()\n",
    "\n",
    "token_range_wdg.value = [0, token_range_wdg.max]\n",
    "text_wdg.observe(update_token_range, 'value')\n",
    "\n",
    "interact = widgets.interact.options(manual_name='Run Lens', manual=True)\n",
    "\n",
    "plot = interact(\n",
    "    make_plot,\n",
    "    text=text_wdg,\n",
    "    statistic=statistic_wdg,\n",
    "    layer_stride=layer_stride_wdg,\n",
    "    token_range=token_range_wdg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dff9aa-8102-49a5-b4ff-6b5ea093cfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
